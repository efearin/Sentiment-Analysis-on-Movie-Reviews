PREPROCESS

-tokenize text (parse them as single words)
	-parse from spaces and separating punctuations
	-specific word grups shouldt be parsed from space in it like "San Francisco"

-lemmatize tokens (put into standard form)
	-number should be written (instead of "7" convert it to "seven")
	-all should be capital or all uncapital
	
-set a vocabulary from the whole set and give them all a unique id
	-to create a vocabulary set delete uninformative words like "the", "an" ...
	-most frequent words could be selected to create a vocabulary list (sense list)
	-all remaining words that couldt be found in vocabulary list will take another specific id so add one more id for the remaning ones to the list
	-id's in convention is noted as "w" (categorical feature for the original word)

--	
WORD REPRESENTATIONS
	
-one hot encoding (to vectorize the word to feed through the any machine learning algorithm)
	-let say vocabulary size is D=10 (means 9 words are in the list and 1 id is specified for remaning out of vocabulary representation) 
		for example for word id 4 (w=4) in the list the corresponding one-hot vector is e(w=4)=[0001000000]
	-one-hot encoding makes no assumption about word similarities such that |e(w)-e(w')|^(2)=0 if ws (words) are same or 2 if words are different
		which means words are equally different from each other everytime
		if we use w's directly the distance brtween for example w=5 and w=100 wouldnt be same 
		one hot gives same distance oppotunity without considering similarities between meanings of the words
	-the problem is one hot coding is very high dimentional since each e() for specific w has the size of whole vocabulary
		for example if vocabulary size is 100 000 for 10 word window size we need 1 000 000 units to extract a meaning from it
	-moreover although one hot vectors are sparse and vector multipication for example done efficiently, 
		computations from the second level when non sparse vectors appears will expensive in terms of logical power
		and reconstruction of the sentence vector forms are hard to do
	-vulnerability to overfitting since high dimentional
		millions of inputs means millions of parameters to train in regular nural network
		
--

-continuous word representation (learn continuous representation of words, each w is associated with a real valued vector C(w))
	-for example the word "the" has id w=1 and C(w=1)=[0.67,0.9,...] such that similar words in meaning has similar vectors means close each other in euclidian distance
		the thing is trying to learn the vectors of words (using neural network)
	-let say window of 10 words [w1,w2...w10] concatenate the vector representation of each word as x=[C(w0)(transpose),C(w1)(transpose),...] and feed that to neural network
	-use stocastic (?) gradient descent
		-dont only update neural net parameters also update C(w) in the x with gradient step as [ C(w) <= C(w)-(alpha)(gradientC(w))l ] l is the loss function optimised by neural net
		
-C matrix has all C(w) vectors as its row 
	-C(w) = e(w)(transpose).C one hot of w and C multiplied to get C(w) 
		bu instead of vector multiplication C(w)s are located to a lookup table seperately

--
LANGUAGE MODELING

-probabilistic model that assign probabilities to any sequence of words
-language modeling is the task of learning a language model that assignshigh probability to well formed sentences
	-for example if we have such model "unepersonne intelligente" translation could be structured as "a smart person" from individual translations of words

--

-an assumption frequently made is nth order markov assumption
	-t'th word is generated based on n previous words such that p(w1,w2..wT)=(Multiply from t=1 through T) p(wt|w(t-(n-1)*...w(t-1))
	-w(t-(n-1)),...,w(t-1) called as context

--
	
-n gram model is a sequence of n words
	-used to calculate conditional probability in markov model
	-unigrams n=1 "is" "a" "second" / bigrams n=2 "is a" "a second" ...
	-p(wt|w(t-(n-1))...w(t-1))=count(w(t-(n-1))...w(t-1))/count(w(t-(n-1))...(all end word including w(t-1)))
	-to be accurate n has to be large but for large n no estimation for initial words up to n word collection created previously
	-also when n is large the probability of finding given sequence in training corpora is decrease so the sequence become not seen before (sparsity)												
		smooting the counts may help in that case
			-combine count(w1,w2..w4) count(w2,w3,w4) count(w3,w4) and count(w4) to calculate p(w4|w1,w2,w3)

--
NEURAL NETWORK LANGUAGE MODEL

-take vectors of each word in the sequence from C matrix rows and feed them to the neural net as matrix X (concatinated word representation vectros in word sequence)
	-feed forward network structure multiply X matrix with a weight and add a bias and nonlinearity (tanh may be)
	-softmax at the output layer and get i'th output=P(wt=i|context)
	-with this not only the neural net weights are trained but also the c(w=) word vector representations are trained
	