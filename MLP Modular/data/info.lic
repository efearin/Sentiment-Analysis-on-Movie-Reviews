291068.1729273796

data_divide_constant = 5
fake_data_divide_constant = 5

length dependent polyfit
common_words_list = freq_list.most_common(500)
eqn = np.polyfit(lengths, scores, 5)
if err_mean < 0.6 and err_variance < 0.8:
inp.append([float(phrase_length),
		float(direct_attribution_sum), float(direct_attribution_count),
		float(closest_attribution_sum), float(closest_attribution_count),
		float(expanded_attribution_sum), float(expanded_attribution_count),
		float(dominant_attribution_sum), float(dominant_attribution_count),
		float(dominant_attribution_variance_sum)])