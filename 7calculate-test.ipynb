{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PURPOSE OF CODE\n",
    "\n",
    "The test set scores are assigned and error is calculated as new columns for all test phrases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROCESS NOTES\n",
    "\n",
    "WILL BE DONE\n",
    "- recursive search for word groups may be written\n",
    "- most dominant words may extracted for better score assginment\n",
    "\n",
    "DONE\n",
    "- test phrase is searched in train set\n",
    "- if no found, in test phrase word group, try to find longest sub phrase that is in train set\n",
    "- if sub phrase found in train set, search for remaining words seperately on train set, if no found get the closest word's sentiment score using w2v model created for each one and average all scores get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\efear\\Anaconda3\\lib\\site-packages\\gensim-2.1.0-py3.6-win-amd64.egg\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Slow version of gensim.models.doc2vec is being used\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gensim, logging\n",
    "import pandas as pd\n",
    "model= gensim.models.Word2Vec.load(\"w2vmodel\")\n",
    "words=pd.read_csv('data/words.csv',sep=\"\\t\")\n",
    "train=pd.read_csv('data/train-clean.csv',sep=\"\\t\")\n",
    "test=pd.read_csv('data/test.csv',sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>calculation</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39654</td>\n",
       "      <td>1893</td>\n",
       "      <td>someth new</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>56036</td>\n",
       "      <td>2802</td>\n",
       "      <td>film editor</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>140518</td>\n",
       "      <td>7625</td>\n",
       "      <td>unusu biopic document</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>129586</td>\n",
       "      <td>6968</td>\n",
       "      <td>hi brawni frame cool compos deliveri fit bill ...</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>104116</td>\n",
       "      <td>5490</td>\n",
       "      <td>skip thi dreck rent anim hous go back sourc</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1  PhraseId  SentenceId  \\\n",
       "0           0             0     39654        1893   \n",
       "1           1             1     56036        2802   \n",
       "2           2             2    140518        7625   \n",
       "3           3             3    129586        6968   \n",
       "4           4             4    104116        5490   \n",
       "\n",
       "                                              Phrase  Sentiment  calculation  \\\n",
       "0                                         someth new          3          NaN   \n",
       "1                                        film editor          2          NaN   \n",
       "2                              unusu biopic document          3          NaN   \n",
       "3  hi brawni frame cool compos deliveri fit bill ...          2          NaN   \n",
       "4        skip thi dreck rent anim hous go back sourc          1          NaN   \n",
       "\n",
       "   error  \n",
       "0    NaN  \n",
       "1    NaN  \n",
       "2    NaN  \n",
       "3    NaN  \n",
       "4    NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['calculation'] = pd.Series(np.NaN , index=test.index)\n",
    "test['error'] = pd.Series(np.NaN , index=test.index)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.core.debugger import Tracer; debug = Tracer()\n",
    "from scipy.spatial import distance\n",
    "  \n",
    "\n",
    "phrase=[]\n",
    "tmp1_phrase=[]\n",
    "tmp2_phrase=[]\n",
    "attribution_count=0\n",
    "attribution_sum=0\n",
    "\n",
    "\n",
    "for indx,row in test.iterrows():\n",
    "    #check if phrase is in string type\n",
    "    if isinstance(row.Phrase,str):\n",
    "        #check if there is something\n",
    "        if len(row.Phrase):\n",
    "            #chech if that phrase is in the train set alreadt\n",
    "            find=train.Phrase[train.Phrase==row.Phrase]\n",
    "            if len(find.index):\n",
    "                #if already in train set assign the score directly\n",
    "                test.loc[indx,\"calculation\"]=train.loc[find.index[0],\"Sentiment\"]\n",
    "            else:\n",
    "                attribution_count=0\n",
    "                attribution_sum=0\n",
    "                phrase=row.Phrase.split()\n",
    "                length=len(phrase)\n",
    "                #if phrase contains single word\n",
    "                if length>1:\n",
    "                    tmp=1\n",
    "                    # with tmp+1<length words could be checked seperately\n",
    "                    while tmp<length:\n",
    "                        for x in range(0,tmp+1):\n",
    "                            #devide phrase into 2 part one is the major word group that will be searched as it is\n",
    "                            #other group words will be scored seperately\n",
    "                            tmp1_phrase=phrase[x:(length-tmp+x)]\n",
    "                            tmp2_phrase=list(set(phrase)-set(tmp1_phrase))\n",
    "                            find=train.Phrase[train.Phrase==\" \".join(tmp1_phrase)]\n",
    "                            a=len(find.index)\n",
    "                            #check if word group is found if found break\n",
    "                            if a:\n",
    "                                break\n",
    "                        #if found calculate score\n",
    "                        if a:\n",
    "                            attribution_count+=1\n",
    "                            attribution_sum+=train.loc[find.index[0],\"Sentiment\"]\n",
    "                            for y in tmp2_phrase:\n",
    "                                find=train.Phrase[train.Phrase==y]\n",
    "                                if len(find.index):\n",
    "                                    attribution_count+=1\n",
    "                                    attribution_sum+=train.loc[find.index[0],\"Sentiment\"]\n",
    "                                else:\n",
    "                                    try:\n",
    "                                        vec=model[y]\n",
    "                                        tmpvec=model[words.loc[0,\"Phrase\"]]\n",
    "                                        tmpindx=0\n",
    "                                        dist=distance.euclidean(vec,tmpvec)\n",
    "                                        for indx1,row in words.iterrows():\n",
    "                                            tmpdist=distance.euclidean(vec,model[row.Phrase])\n",
    "                                            if tmpdist<dist:\n",
    "                                                dist=tmpdist\n",
    "                                                tmpvec=model[row.Phrase]\n",
    "                                                tmpindx=indx1\n",
    "                                        attribution_count+=1\n",
    "                                        attribution_sum+=words.loc[tmpindx,\"Sentiment\"]\n",
    "                                    except:\n",
    "                                        pass\n",
    "                            break\n",
    "                        tmp+=1\n",
    "                #if single word phrase calculate score\n",
    "                else:\n",
    "                    try:\n",
    "                        vec=model[row.Phrase]\n",
    "                        tmpvec=model[words.loc[0,\"Phrase\"]]\n",
    "                        tmpindx=0\n",
    "                        dist=distance.euclidean(vec,tmpvec)\n",
    "                        for indx1,row in words.iterrows():\n",
    "                            tmpdist=distance.euclidean(vec,model[row.Phrase])\n",
    "                            if tmpdist<dist:\n",
    "                                dist=tmpdist\n",
    "                                tmpvec=model[row.Phrase]\n",
    "                                tmpindx=indx1\n",
    "                        attribution_count+=1\n",
    "                        attribution_sum+=words.loc[tmpindx,\"Sentiment\"]\n",
    "                    except:\n",
    "                        pass\n",
    "                if attribution_count!=0:\n",
    "                    test.loc[indx,\"calculation\"]=attribution_sum/attribution_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for indx,row in test.iterrows():\n",
    "    test.loc[indx,\"error\"]= abs(row.Sentiment-row.calculation)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = test.drop('Unnamed: 0', 1)\n",
    "test.to_csv(\"data/test-calculated.csv\", sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
