{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean data using \"1clean-data.ipynb\"\n",
    "Takes input \"../data/data.csv\"\n",
    "Outputs a \"../data/clean-data.csv\" file. \n",
    "For details see \"1clean-data.ipynb\" itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done succesfully\n"
     ]
    }
   ],
   "source": [
    "%run \"1clean-data.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Devide data using \"2devide-data.ipynb\"\n",
    "Takes input \"../data/clean-data.csv\" file.\n",
    "Outputs \"../data/1df.csv\", \"../data/2df.csv\" ... \"../data/5df.csv\"\n",
    "For details see \"2devide-data.ipynb\" itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done succesfully\n"
     ]
    }
   ],
   "source": [
    "%run \"2devide-data.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "path=os.getcwd()+\"\\\\data\\\\\"\n",
    "\n",
    "dfset=[]\n",
    "for x in range (1,6):\n",
    "    dfset.append(pd.read_csv(path+str(x)+\"df.csv\",sep=\"\\t\"))\n",
    "\n",
    "test=dfset[0].drop('Unnamed: 0', 1).reset_index(drop=True)\n",
    "del dfset[0]\n",
    "train=pd.concat(dfset).drop('Unnamed: 0', 1).reset_index(drop=True)\n",
    "train.to_csv(\"data/train.csv\", sep='\\t')\n",
    "test.to_csv(\"data/test.csv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done succesfully\n"
     ]
    }
   ],
   "source": [
    "%run \"3clean-train.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare train data for w2v.\n",
    "Takes input \"../data/train.csv\"\n",
    "Outputs \"../data/trainw2v.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done succesfully\n"
     ]
    }
   ],
   "source": [
    "%run \"4get-sentences.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\efear\\Anaconda3\\lib\\site-packages\\gensim-2.1.0-py3.6-win-amd64.egg\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Slow version of gensim.models.doc2vec is being used\n",
      "2017-06-15 17:05:40,724 : WARNING : Slow version of gensim.models.word2vec is being used\n",
      "2017-06-15 17:05:40,725 : INFO : collecting all words and their counts\n",
      "2017-06-15 17:05:40,727 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-06-15 17:05:40,747 : INFO : collected 11658 word types from a corpus of 84512 raw words and 8416 sentences\n",
      "2017-06-15 17:05:40,749 : INFO : Loading a fresh vocabulary\n",
      "2017-06-15 17:05:40,777 : INFO : min_count=1 retains 11658 unique words (100% of original 11658, drops 0)\n",
      "2017-06-15 17:05:40,778 : INFO : min_count=1 leaves 84512 word corpus (100% of original 84512, drops 0)\n",
      "2017-06-15 17:05:40,815 : INFO : deleting the raw counts dictionary of 11658 items\n",
      "2017-06-15 17:05:40,816 : INFO : sample=0.001 downsamples 28 most-common words\n",
      "2017-06-15 17:05:40,818 : INFO : downsampling leaves estimated 78702 word corpus (93.1% of prior 84512)\n",
      "2017-06-15 17:05:40,820 : INFO : estimated required memory for 11658 words and 100 dimensions: 15155400 bytes\n",
      "2017-06-15 17:05:40,853 : INFO : resetting layer weights\n",
      "C:\\Users\\efear\\Anaconda3\\lib\\site-packages\\gensim-2.1.0-py3.6-win-amd64.egg\\gensim\\models\\word2vec.py:789: UserWarning: C extension not loaded for Word2Vec, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "  warnings.warn(\"C extension not loaded for Word2Vec, training will be slow. \"\n",
      "2017-06-15 17:05:41,019 : INFO : training model with 3 workers on 11658 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-06-15 17:05:49,716 : INFO : PROGRESS: at 2.40% examples, 1069 words/s, in_qsize 5, out_qsize 0\n",
      "2017-06-15 17:05:58,283 : INFO : PROGRESS: at 9.77% examples, 2159 words/s, in_qsize 5, out_qsize 0\n",
      "2017-06-15 17:06:06,167 : INFO : PROGRESS: at 16.75% examples, 2593 words/s, in_qsize 6, out_qsize 0\n",
      "2017-06-15 17:06:13,980 : INFO : PROGRESS: at 23.80% examples, 2826 words/s, in_qsize 5, out_qsize 0\n",
      "2017-06-15 17:06:15,044 : INFO : PROGRESS: at 28.63% examples, 3285 words/s, in_qsize 5, out_qsize 0\n",
      "2017-06-15 17:06:21,890 : INFO : PROGRESS: at 30.98% examples, 2963 words/s, in_qsize 6, out_qsize 0\n",
      "2017-06-15 17:06:23,079 : INFO : PROGRESS: at 35.72% examples, 3321 words/s, in_qsize 5, out_qsize 0\n",
      "2017-06-15 17:06:30,093 : INFO : PROGRESS: at 37.95% examples, 3036 words/s, in_qsize 6, out_qsize 0\n",
      "2017-06-15 17:06:31,519 : INFO : PROGRESS: at 40.21% examples, 3134 words/s, in_qsize 5, out_qsize 0\n",
      "2017-06-15 17:06:40,172 : INFO : PROGRESS: at 45.15% examples, 2990 words/s, in_qsize 6, out_qsize 0\n",
      "2017-06-15 17:06:41,982 : INFO : PROGRESS: at 47.54% examples, 3054 words/s, in_qsize 5, out_qsize 0\n",
      "2017-06-15 17:06:51,048 : INFO : PROGRESS: at 52.32% examples, 2925 words/s, in_qsize 6, out_qsize 0\n",
      "2017-06-15 17:06:52,599 : INFO : PROGRESS: at 54.70% examples, 2991 words/s, in_qsize 5, out_qsize 0\n",
      "2017-06-15 17:07:00,219 : INFO : PROGRESS: at 59.18% examples, 2939 words/s, in_qsize 5, out_qsize 0\n",
      "2017-06-15 17:07:01,994 : INFO : PROGRESS: at 61.60% examples, 2989 words/s, in_qsize 6, out_qsize 0\n",
      "2017-06-15 17:07:09,513 : INFO : PROGRESS: at 66.46% examples, 2946 words/s, in_qsize 6, out_qsize 0\n",
      "2017-06-15 17:07:11,473 : INFO : PROGRESS: at 68.82% examples, 2985 words/s, in_qsize 5, out_qsize 0\n",
      "2017-06-15 17:07:18,519 : INFO : PROGRESS: at 73.62% examples, 2960 words/s, in_qsize 6, out_qsize 0\n",
      "2017-06-15 17:07:20,531 : INFO : PROGRESS: at 75.93% examples, 2994 words/s, in_qsize 6, out_qsize 0\n",
      "2017-06-15 17:07:27,196 : INFO : PROGRESS: at 80.44% examples, 2981 words/s, in_qsize 5, out_qsize 0\n",
      "2017-06-15 17:07:29,605 : INFO : PROGRESS: at 82.92% examples, 3001 words/s, in_qsize 6, out_qsize 0\n",
      "2017-06-15 17:07:36,627 : INFO : PROGRESS: at 87.74% examples, 2979 words/s, in_qsize 6, out_qsize 0\n",
      "2017-06-15 17:07:39,160 : INFO : PROGRESS: at 90.11% examples, 2994 words/s, in_qsize 5, out_qsize 0\n",
      "2017-06-15 17:07:44,917 : INFO : PROGRESS: at 94.89% examples, 3005 words/s, in_qsize 3, out_qsize 0\n",
      "2017-06-15 17:07:47,110 : INFO : PROGRESS: at 95.50% examples, 2973 words/s, in_qsize 2, out_qsize 1\n",
      "2017-06-15 17:07:47,112 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-06-15 17:07:47,288 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-06-15 17:07:47,450 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-06-15 17:07:47,452 : INFO : training on 422560 raw words (393513 effective words) took 126.4s, 3113 effective words/s\n",
      "2017-06-15 17:07:47,453 : INFO : saving Word2Vec object under w2vmodel, separately None\n",
      "2017-06-15 17:07:47,455 : INFO : not storing attribute syn0norm\n",
      "2017-06-15 17:07:47,456 : INFO : not storing attribute cum_table\n",
      "2017-06-15 17:07:47,573 : INFO : saved w2vmodel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done succesfully\n"
     ]
    }
   ],
   "source": [
    "%run \"5get-w2v-model.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run \"6get-words.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-06-15 17:07:47,824 : INFO : loading Word2Vec object from w2vmodel\n",
      "2017-06-15 17:07:47,921 : INFO : loading wv recursively from w2vmodel.wv.* with mmap=None\n",
      "2017-06-15 17:07:47,922 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-06-15 17:07:47,923 : INFO : setting ignored attribute cum_table to None\n",
      "2017-06-15 17:07:47,925 : INFO : loaded w2vmodel\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gensim, logging\n",
    "model= gensim.models.Word2Vec.load(\"w2vmodel\")\n",
    "words=pd.read_csv('data/words.csv',sep=\"\\t\")\n",
    "train=pd.read_csv('data/train-clean.csv',sep=\"\\t\")\n",
    "test=pd.read_csv('data/test.csv',sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>calculation</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>37156</td>\n",
       "      <td>1762</td>\n",
       "      <td>idea lrb middleag romanc rrb not handl well</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>64520</td>\n",
       "      <td>3265</td>\n",
       "      <td>dispens</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>39985</td>\n",
       "      <td>1911</td>\n",
       "      <td>stockwel</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>151417</td>\n",
       "      <td>8255</td>\n",
       "      <td>deliveri</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>7614</td>\n",
       "      <td>308</td>\n",
       "      <td>airhead movi busi deserv right</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  PhraseId  SentenceId  \\\n",
       "0           0     37156        1762   \n",
       "1           1     64520        3265   \n",
       "2           2     39985        1911   \n",
       "3           3    151417        8255   \n",
       "4           4      7614         308   \n",
       "\n",
       "                                        Phrase  Sentiment  calculation  error  \n",
       "0  idea lrb middleag romanc rrb not handl well          1          NaN    NaN  \n",
       "1                                      dispens          2          NaN    NaN  \n",
       "2                                     stockwel          2          NaN    NaN  \n",
       "3                                     deliveri          2          NaN    NaN  \n",
       "4               airhead movi busi deserv right          1          NaN    NaN  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['calculation'] = pd.Series(np.NaN , index=test.index)\n",
    "test['error'] = pd.Series(np.NaN , index=test.index)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sadece kesme yerlerini belirle iki integerla  ayırdığı parça 3 tane böylece iki tarafına da bakabilirsin böylece yarı uzunluğa kadar bakabilirsin diğer iki taraf için de aynı algoritmayı çağırabilirsin o da ii olur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-e68fb7cf48bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mattribution_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mattribution_sum\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mphrase\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPhrase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mlength\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphrase\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlength\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "from IPython.core.debugger import Tracer; debug = Tracer()\n",
    "from scipy.spatial import distance\n",
    "  \n",
    "\n",
    "phrase=[]\n",
    "tmp1_phrase=[]\n",
    "tmp2_phrase=[]\n",
    "attribution_count=0\n",
    "attribution_sum=0\n",
    "\n",
    "\n",
    "for indx,row in test.iterrows():\n",
    "    find=train.Phrase[train.Phrase==row.Phrase]\n",
    "    if len(find.index):\n",
    "        test.loc[indx,\"calculation\"]=train.loc[find.index[0],\"Sentiment\"]\n",
    "    else:\n",
    "        attribution_count=0\n",
    "        attribution_sum=0\n",
    "        phrase=row.Phrase.split()\n",
    "        length=len(phrase)\n",
    "        if length>1:\n",
    "            tmp=1\n",
    "            # with tmp+1<length words could be checked seperately\n",
    "            while tmp<length:\n",
    "                for x in range(0,tmp+1):\n",
    "                    tmp1_phrase=phrase[x:(length-tmp+x)]\n",
    "                    tmp2_phrase=list(set(phrase)-set(tmp1_phrase))\n",
    "                    find=train.Phrase[train.Phrase==\" \".join(tmp1_phrase)]\n",
    "                    a=len(find.index)\n",
    "                    if a:\n",
    "                        break\n",
    "                if a:\n",
    "                    attribution_count+=1\n",
    "                    attribution_sum+=train.loc[find.index[0],\"Sentiment\"]\n",
    "                    for y in tmp2_phrase:\n",
    "                        find=train.Phrase[train.Phrase==y]\n",
    "                        if len(find.index):\n",
    "                            attribution_count+=1\n",
    "                            attribution_sum+=train.loc[find.index[0],\"Sentiment\"]\n",
    "                        else:\n",
    "                            try:\n",
    "                                vec=model[y]\n",
    "                                tmpvec=model[words.loc[0,\"Phrase\"]]\n",
    "                                tmpindx=0\n",
    "                                dist=distance.euclidean(vec,tmpvec)\n",
    "                                for indx1,row in words.iterrows():\n",
    "                                    tmpdist=distance.euclidean(vec,model[row.Phrase])\n",
    "                                    if tmpdist<dist:\n",
    "                                        dist=tmpdist\n",
    "                                        tmpvec=model[row.Phrase]\n",
    "                                        tmpindx=indx1\n",
    "                                attribution_count+=1\n",
    "                                attribution_sum+=words.loc[tmpindx,\"Sentiment\"]\n",
    "                            except:\n",
    "                                pass\n",
    "                    break\n",
    "                tmp+=1\n",
    "        else:\n",
    "            try:\n",
    "                vec=model[row.Phrase]\n",
    "                tmpvec=model[words.loc[0,\"Phrase\"]]\n",
    "                tmpindx=0\n",
    "                dist=distance.euclidean(vec,tmpvec)\n",
    "                for indx1,row in words.iterrows():\n",
    "                    tmpdist=distance.euclidean(vec,model[row.Phrase])\n",
    "                    if tmpdist<dist:\n",
    "                        dist=tmpdist\n",
    "                        tmpvec=model[row.Phrase]\n",
    "                        tmpindx=indx1\n",
    "                attribution_count+=1\n",
    "                attribution_sum+=words.loc[tmpindx,\"Sentiment\"]\n",
    "            except:\n",
    "                pass\n",
    "        if attribution_count!=0:\n",
    "            test.loc[indx,\"calculation\"]=attribution_sum/attribution_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for indx,row in test.iterrows():\n",
    "    test.loc[indx,\"error\"]= abs(row.Sentiment-row.calculation)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test.error.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
