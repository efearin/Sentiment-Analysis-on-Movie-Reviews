{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean data using \"1clean-data.ipynb\"\n",
    "Takes input \"../data/data.csv\"\n",
    "Outputs a \"../data/clean-data.csv\" file. \n",
    "For details see \"1clean-data.ipynb\" itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run \"1clean-data.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Devide data using \"2devide-data.ipynb\"\n",
    "Takes input \"../data/clean-data.csv\" file.\n",
    "Outputs \"../data/1df.csv\", \"../data/2df.csv\" ... \"../data/5df.csv\"\n",
    "For details see \"2devide-data.ipynb\" itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run \"2devide-data.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "path=os.getcwd()+\"\\\\data\\\\\"\n",
    "\n",
    "dfset=[]\n",
    "for x in range (1,6):\n",
    "    dfset.append(pd.read_csv(path+str(x)+\"df.csv\",sep=\"\\t\"))\n",
    "\n",
    "test=dfset[0].drop('Unnamed: 0', 1).reset_index(drop=True)\n",
    "del dfset[0]\n",
    "train=pd.concat(dfset).drop('Unnamed: 0', 1).reset_index(drop=True)\n",
    "train.to_csv(\"data/train.csv\", sep='\\t')\n",
    "test.to_csv(\"data/test.csv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare train data for w2v.\n",
    "Takes input \"../data/train.csv\"\n",
    "Outputs \"../data/trainw2v.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run \"3get-sentences.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainw2v=pd.read_csv('data/trainw2v.csv',sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize sentences to feed w2v."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "tokenized_sentences=trainw2v.apply(lambda row: word_tokenize(row['Phrase']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a model of trained w2v and save it to \"../w2vmodel\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\efear\\Anaconda3\\lib\\site-packages\\gensim-2.1.0-py3.6-win-amd64.egg\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Slow version of gensim.models.doc2vec is being used\n",
      "2017-06-11 12:14:38,759 : WARNING : Slow version of gensim.models.word2vec is being used\n",
      "2017-06-11 12:14:38,761 : INFO : collecting all words and their counts\n",
      "2017-06-11 12:14:38,766 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-06-11 12:14:38,809 : INFO : collected 11565 word types from a corpus of 82297 raw words and 8346 sentences\n",
      "2017-06-11 12:14:38,812 : INFO : Loading a fresh vocabulary\n",
      "2017-06-11 12:14:38,934 : INFO : min_count=1 retains 11565 unique words (100% of original 11565, drops 0)\n",
      "2017-06-11 12:14:38,935 : INFO : min_count=1 leaves 82297 word corpus (100% of original 82297, drops 0)\n",
      "2017-06-11 12:14:39,026 : INFO : deleting the raw counts dictionary of 11565 items\n",
      "2017-06-11 12:14:39,030 : INFO : sample=0.001 downsamples 29 most-common words\n",
      "2017-06-11 12:14:39,033 : INFO : downsampling leaves estimated 76673 word corpus (93.2% of prior 82297)\n",
      "2017-06-11 12:14:39,037 : INFO : estimated required memory for 11565 words and 100 dimensions: 15034500 bytes\n",
      "2017-06-11 12:14:39,106 : INFO : resetting layer weights\n",
      "C:\\Users\\efear\\Anaconda3\\lib\\site-packages\\gensim-2.1.0-py3.6-win-amd64.egg\\gensim\\models\\word2vec.py:789: UserWarning: C extension not loaded for Word2Vec, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "  warnings.warn(\"C extension not loaded for Word2Vec, training will be slow. \"\n",
      "2017-06-11 12:14:39,348 : INFO : training model with 3 workers on 11565 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-06-11 12:14:49,335 : INFO : PROGRESS: at 2.42% examples, 934 words/s, in_qsize 5, out_qsize 0\n",
      "2017-06-11 12:14:59,470 : INFO : PROGRESS: at 9.57% examples, 1854 words/s, in_qsize 5, out_qsize 0\n",
      "2017-06-11 12:15:08,898 : INFO : PROGRESS: at 16.93% examples, 2207 words/s, in_qsize 5, out_qsize 0\n",
      "2017-06-11 12:15:16,575 : INFO : PROGRESS: at 24.24% examples, 2502 words/s, in_qsize 5, out_qsize 0\n",
      "2017-06-11 12:15:24,117 : INFO : PROGRESS: at 31.46% examples, 2705 words/s, in_qsize 6, out_qsize 0\n",
      "2017-06-11 12:15:25,313 : INFO : PROGRESS: at 36.36% examples, 3039 words/s, in_qsize 6, out_qsize 0\n",
      "2017-06-11 12:15:31,805 : INFO : PROGRESS: at 38.78% examples, 2840 words/s, in_qsize 5, out_qsize 0\n",
      "2017-06-11 12:15:33,178 : INFO : PROGRESS: at 43.70% examples, 3113 words/s, in_qsize 5, out_qsize 0\n",
      "2017-06-11 12:15:39,344 : INFO : PROGRESS: at 46.04% examples, 2949 words/s, in_qsize 5, out_qsize 0\n",
      "2017-06-11 12:15:40,902 : INFO : PROGRESS: at 50.87% examples, 3177 words/s, in_qsize 6, out_qsize 0\n",
      "2017-06-11 12:15:46,850 : INFO : PROGRESS: at 53.35% examples, 3035 words/s, in_qsize 5, out_qsize 0\n",
      "2017-06-11 12:15:48,490 : INFO : PROGRESS: at 58.22% examples, 3232 words/s, in_qsize 5, out_qsize 0\n",
      "2017-06-11 12:15:54,300 : INFO : PROGRESS: at 60.70% examples, 3106 words/s, in_qsize 6, out_qsize 0\n",
      "2017-06-11 12:15:56,144 : INFO : PROGRESS: at 65.53% examples, 3273 words/s, in_qsize 6, out_qsize 0\n",
      "2017-06-11 12:16:01,743 : INFO : PROGRESS: at 67.92% examples, 3164 words/s, in_qsize 5, out_qsize 0\n",
      "2017-06-11 12:16:03,858 : INFO : PROGRESS: at 72.77% examples, 3305 words/s, in_qsize 5, out_qsize 0\n",
      "2017-06-11 12:16:09,205 : INFO : PROGRESS: at 75.20% examples, 3212 words/s, in_qsize 6, out_qsize 0\n",
      "2017-06-11 12:16:10,282 : INFO : PROGRESS: at 77.66% examples, 3276 words/s, in_qsize 6, out_qsize 0\n",
      "2017-06-11 12:16:11,512 : INFO : PROGRESS: at 80.13% examples, 3333 words/s, in_qsize 6, out_qsize 0\n",
      "2017-06-11 12:16:16,521 : INFO : PROGRESS: at 82.56% examples, 3257 words/s, in_qsize 6, out_qsize 0\n",
      "2017-06-11 12:16:17,862 : INFO : PROGRESS: at 84.95% examples, 3307 words/s, in_qsize 6, out_qsize 0\n",
      "2017-06-11 12:16:19,155 : INFO : PROGRESS: at 87.35% examples, 3358 words/s, in_qsize 6, out_qsize 0\n",
      "2017-06-11 12:16:24,047 : INFO : PROGRESS: at 89.72% examples, 3290 words/s, in_qsize 5, out_qsize 0\n",
      "2017-06-11 12:16:25,446 : INFO : PROGRESS: at 92.21% examples, 3335 words/s, in_qsize 4, out_qsize 0\n",
      "2017-06-11 12:16:26,855 : INFO : PROGRESS: at 94.60% examples, 3378 words/s, in_qsize 3, out_qsize 0\n",
      "2017-06-11 12:16:28,214 : INFO : PROGRESS: at 95.04% examples, 3350 words/s, in_qsize 2, out_qsize 1\n",
      "2017-06-11 12:16:28,216 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-06-11 12:16:30,202 : INFO : PROGRESS: at 97.52% examples, 3374 words/s, in_qsize 1, out_qsize 1\n",
      "2017-06-11 12:16:30,204 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-06-11 12:16:30,593 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-06-11 12:16:30,594 : INFO : training on 411485 raw words (383316 effective words) took 111.2s, 3447 effective words/s\n",
      "2017-06-11 12:16:30,596 : INFO : saving Word2Vec object under w2vmodel, separately None\n",
      "2017-06-11 12:16:30,599 : INFO : not storing attribute syn0norm\n",
      "2017-06-11 12:16:30,601 : INFO : not storing attribute cum_table\n",
      "2017-06-11 12:16:30,724 : INFO : saved w2vmodel\n"
     ]
    }
   ],
   "source": [
    "import gensim, logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "model = gensim.models.Word2Vec(tokenized_sentences, min_count=1)\n",
    "model.save('w2vmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run \"4get-words.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words=pd.read_csv('data/words.csv',sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
